{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "%run knn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def data_processing():\n",
    "    data = pd.read_csv('heart_disease.csv', low_memory=False, sep=',', na_values='?').values\n",
    "\n",
    "    N = data.shape[0]\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    # prepare data\n",
    "\n",
    "    ntr = int(np.round(N * 0.8))\n",
    "    nval = int(np.round(N * 0.15))\n",
    "    ntest = N - ntr - nval\n",
    "\n",
    "    # spliting training, validation, and test\n",
    "    x_train = np.append([np.ones(ntr)], data[:ntr].T[:-1], axis=0).T\n",
    "    y_train = data[:ntr].T[-1].T\n",
    "    x_val = np.append([np.ones(nval)], data[ntr:ntr + nval].T[:-1], axis=0).T\n",
    "    y_val = data[ntr:ntr + nval].T[-1].T\n",
    "    x_test = np.append([np.ones(ntest)], data[-ntest:].T[:-1], axis=0).T\n",
    "    y_test = data[-ntest:].T[-1].T\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "import numpy as np\n",
    "from knn import KNN\n",
    "\n",
    "############################################################################\n",
    "# DO NOT MODIFY ABOVE CODES\n",
    "############################################################################\n",
    "\n",
    "\n",
    "# TODO: implement F1 score\n",
    "def f1_score(real_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Information on F1 score - https://en.wikipedia.org/wiki/F1_score\n",
    "    :param real_labels: List[int]\n",
    "    :param predicted_labels: List[int]\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(real_labels) == len(predicted_labels)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "class Distances:\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def minkowski_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        Minkowski distance is the generalized version of Euclidean Distance\n",
    "        It is also know as L-p norm (where p>=1) that you have studied in class\n",
    "        For our assignment we need to take p=3\n",
    "        Information on Minkowski distance - https://en.wikipedia.org/wiki/Minkowski_distance\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def euclidean_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def inner_product_distance(point1, point2):\n",
    "        \"\"\"\n",
    "        :param point1: List[float]\n",
    "        :param point2: List[float]\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def cosine_similarity_distance(point1, point2):\n",
    "        \"\"\"\n",
    "       :param point1: List[float]\n",
    "       :param point2: List[float]\n",
    "       :return: float\n",
    "       \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    # TODO\n",
    "    def gaussian_kernel_distance(point1, point2):\n",
    "        \"\"\"\n",
    "       :param point1: List[float]\n",
    "       :param point2: List[float]\n",
    "       :return: float\n",
    "       \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        self.best_k = None\n",
    "        self.best_distance_function = None\n",
    "        self.best_scaler = None\n",
    "        self.best_model = None\n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset\n",
    "    def tuning_without_scaling(self, distance_funcs, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        In this part, you should try different distance function you implemented in part 1.1, and find the best k.\n",
    "        Use k range from 1 to 30 and increment by 2. Use f1-score to compare different models.\n",
    "\n",
    "        :param distance_funcs: dictionary of distance functions you must use to calculate the distance.\n",
    "            Make sure you loop over all distance functions for each data point and each k value.\n",
    "            You can refer to test.py file to see the format in which these functions will be\n",
    "            passed by the grading script\n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "        :param y_train: List[int] train labels to train your KNN model\n",
    "        :param x_val:  List[List[int]] Validation data set will be used on your KNN predict function to produce\n",
    "            predicted labels and tune k and distance function.\n",
    "        :param y_val: List[int] validation labels\n",
    "\n",
    "        Find(tune) best k, distance_function and model (an instance of KNN) and assign to self.best_k,\n",
    "        self.best_distance_function and self.best_model respectively.\n",
    "        NOTE: self.best_scaler will be None\n",
    "\n",
    "        NOTE: When there is a tie, choose model based on the following priorities:\n",
    "        Then check distance function  [euclidean > minkowski > gaussian > inner_prod > cosine_dist]\n",
    "        If they have same distance fuction, choose model which has a less k.\n",
    "        \"\"\"\n",
    "        \n",
    "        # You need to assign the final values to these variables\n",
    "        self.best_k = None\n",
    "        self.best_distance_function = None\n",
    "        self.best_model = None\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset, with normalized data\n",
    "    def tuning_with_scaling(self, distance_funcs, scaling_classes, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        This part is similar to Part 1.3 except that before passing your training and validation data to KNN model to\n",
    "        tune k and disrance function, you need to create the normalized data using these two scalers to transform your\n",
    "        data, both training and validation. Again, we will use f1-score to compare different models.\n",
    "        Here we have 3 hyperparameters i.e. k, distance_function and scaler.\n",
    "\n",
    "        :param distance_funcs: dictionary of distance funtions you use to calculate the distance. Make sure you\n",
    "            loop over all distance function for each data point and each k value.\n",
    "            You can refer to test.py file to see the format in which these functions will be\n",
    "            passed by the grading script\n",
    "        :param scaling_classes: dictionary of scalers you will use to normalized your data.\n",
    "        Refer to test.py file to check the format.\n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "        :param y_train: List[int] train labels to train your KNN model\n",
    "        :param x_val: List[List[int]] validation data set you will use on your KNN predict function to produce predicted\n",
    "            labels and tune your k, distance function and scaler.\n",
    "        :param y_val: List[int] validation labels\n",
    "\n",
    "        Find(tune) best k, distance_funtion, scaler and model (an instance of KNN) and assign to self.best_k,\n",
    "        self.best_distance_function, self.best_scaler and self.best_model respectively\n",
    "\n",
    "        NOTE: When there is a tie, choose model based on the following priorities:\n",
    "        For normalization, [min_max_scale > normalize];\n",
    "        Then check distance function  [euclidean > minkowski > gaussian > inner_prod > cosine_dist]\n",
    "        If they have same distance function, choose model which has a less k.\n",
    "        \"\"\"\n",
    "        \n",
    "        # You need to assign the final values to these variables\n",
    "        self.best_k = None\n",
    "        self.best_distance_function = None\n",
    "        self.best_scaler = None\n",
    "        self.best_model = None\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NormalizationScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO: normalize data\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Normalize features for every sample\n",
    "\n",
    "        Example\n",
    "        features = [[3, 4], [1, -1], [0, 0]]\n",
    "        return [[0.6, 0.8], [0.707107, -0.707107], [0, 0]]\n",
    "\n",
    "        :param features: List[List[float]]\n",
    "        :return: List[List[float]]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MinMaxScaler:\n",
    "    \"\"\"\n",
    "    Please follow this link to know more about min max scaling\n",
    "    https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    You should keep some states inside the object.\n",
    "    You can assume that the parameter of the first __call__\n",
    "    will be the training set.\n",
    "\n",
    "    Hints:\n",
    "        1. Use a variable to check for first __call__ and only compute\n",
    "            and store min/max in that case.\n",
    "\n",
    "    Note:\n",
    "        1. You may assume the parameters are valid when __call__\n",
    "            is being called the first time (you can find min and max).\n",
    "\n",
    "    Example:\n",
    "        train_features = [[0, 10], [2, 0]]\n",
    "        test_features = [[20, 1]]\n",
    "\n",
    "        scaler1 = MinMaxScale()\n",
    "        train_features_scaled = scaler1(train_features)\n",
    "        # train_features_scaled should be equal to [[0, 1], [1, 0]]\n",
    "\n",
    "        test_features_scaled = scaler1(test_features)\n",
    "        # test_features_scaled should be equal to [[10, 0.1]]\n",
    "\n",
    "        new_scaler = MinMaxScale() # creating a new scaler\n",
    "        _ = new_scaler([[1, 1], [0, 0]]) # new trainfeatures\n",
    "        test_features_scaled = new_scaler(test_features)\n",
    "        # now test_features_scaled should be [[20, 1]]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        normalize the feature vector for each sample . For example,\n",
    "        if the input features = [[2, -1], [-1, 5], [0, 0]],\n",
    "        the output should be [[1, 0], [0, 1], [0.333333, 0.16667]]\n",
    "\n",
    "        :param features: List[List[float]]\n",
    "        :return: List[List[float]]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
